# IL + RL + Reflexion Configuration

experiment:
  name: "il_rl_reflexion"
  description: "CQL with VLM-generated hints injected into instructions"
  
policy:
  type: "rl_cql_reflexion"
  checkpoint: "models/offline_rl_cql/cql_policy.d3"
  use_rl: true
  use_reflexion: true
  
reflexion:
  vlm_model: "Qwen/Qwen2-VL-7B-Instruct"
  rule_database: "data/rules/rule_database.json"
  top_k_rules: 3
  hint_injection: true
  
evaluation:
  script: "rl/eval_with_reflexion.py"
  splits:
    - "valid_seen"
    - "valid_unseen"
  num_episodes_per_split: 85
  max_steps_per_episode: 100
  
metrics:
  output_file: "data/logs/il_rl_reflexion_sim_metrics.csv"
  track:
    - "action_accuracy"
    - "success_rate"
    - "average_episode_length"
    - "num_episodes"
    - "reflexion_improvement"
